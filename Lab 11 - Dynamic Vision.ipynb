{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bc57d1-e9d8-4129-8e5b-f0ea61c49327",
   "metadata": {},
   "source": [
    "# 2110433 - Computer Vision (2024/2)\n",
    "## Lab 11 - Dynamic Vision & 3D\n",
    "In this lab, we will learn to describe image motion from images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f51e97-46d8-4886-a443-bf654c310ecf",
   "metadata": {},
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a6184-9bca-4eed-80a6-722b27f1a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be31d0-b286-4a28-8307-fbecec33526c",
   "metadata": {},
   "source": [
    "## 1. Background Subtraction\n",
    "This tutorial was originally from [here](https://docs.opencv.org/3.4/d1/dc5/tutorial_background_subtraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a1153-ae12-42d9-9f28-511bc5bbc4fe",
   "metadata": {},
   "source": [
    "- Background subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n",
    "- As the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259566ce-4415-43dc-8efa-b00723a7f2e7",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.opencv.org/3.4/Background_Subtraction_Tutorial_Scheme.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26baa5b-008c-4786-93b0-7607b67659e1",
   "metadata": {},
   "source": [
    "A [cv2.VideoCapture](https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html) object is used to read the input video or input images sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536818dc-ebc1-44e7-affd-6cf4bfbc36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f84c27-b216-4545-b444-c726b9435443",
   "metadata": {},
   "source": [
    "A [cv2.BackgroundSubtractor](https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.html) object will be used to generate the foreground mask. In this example, default parameters are used, but it is also possible to declare specific parameters in the create function.\n",
    "\n",
    "We use MOG background subtractor for this example.\n",
    "\n",
    "It uses a method to model each background pixel by a mixture of K Gaussian distributions (K = 3 to 5). The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and more static.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*1EkPhFtPTRgmHumI.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b191306-0341-4489-b2cd-218eef02daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "backSub = cv2.bgsegm.createBackgroundSubtractorMOG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8639bab-b16d-4442-b7ae-601bb4f029e5",
   "metadata": {},
   "source": [
    "Every frame is used both for calculating the foreground mask and for updating the background. If you want to change the learning rate used for updating the background model, it is possible to set a specific learning rate by passing a parameter to the apply method.\n",
    "\n",
    "You can read more about \"apply\" function by clicking [here](https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.html#aa735e76f7069b3fa9c3f32395f9ccd21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5d56d-b6ab-4fc7-862f-23127f621402",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = capture.read()\n",
    "fgMask = backSub.apply(frame)\n",
    "plt.imshow(fgMask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a52cf0-4bce-43b2-9662-5ab6678cd434",
   "metadata": {},
   "source": [
    "Show the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6c322-412f-44d9-996d-e99d34070e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')\n",
    "backSub = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    fgMask = backSub.apply(frame)\n",
    "    \n",
    "    \n",
    "    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv2.putText(frame, str(capture.get(cv2.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('KNN Mask', fgMask)\n",
    "    \n",
    "    keyboard = cv2.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91042a90-c3bf-4d6b-b485-dd0767accdd2",
   "metadata": {},
   "source": [
    "There are other background subtraction algorithms as shown in the image below.\n",
    "\n",
    "<img src=\"https://docs.opencv.org/3.4/d7/df6/classcv_1_1BackgroundSubtractor.png\"></img>\n",
    "\n",
    "In the next block, we compare 4 background subtraction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e73c8-967c-45ca-b941-0b9d5a1d2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('assets/vtest.avi')\n",
    "\n",
    "\n",
    "backSubKNN = cv2.createBackgroundSubtractorKNN()\n",
    "backSubMOG2 = cv2.createBackgroundSubtractorMOG2()\n",
    "backSubMOG = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "backSubGMG = cv2.bgsegm.createBackgroundSubtractorGMG()\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, (400, 300)) \n",
    "    \n",
    "    fgMaskKNN = backSubKNN.apply(frame)\n",
    "    fgMaskMOG = backSubMOG.apply(frame)\n",
    "    fgMaskMOG2 = backSubMOG2.apply(frame)\n",
    "    fgMaskGMG = backSubGMG.apply(frame)\n",
    "    \n",
    "    \n",
    "    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv2.putText(frame, str(capture.get(cv2.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    \n",
    "    \n",
    "    original =  cv2.imshow('Frame', frame)\n",
    "    \n",
    "    \n",
    "    cv2.imshow('KNN Mask', fgMaskKNN)\n",
    "    cv2.imshow('MOG Mask', fgMaskMOG)\n",
    "    cv2.imshow('MOG2 Mask', fgMaskMOG2)\n",
    "    cv2.imshow('GMG Mask', fgMaskGMG)\n",
    "    \n",
    "    keyboard = cv2.waitKey(30)\n",
    "    if keyboard == 'q' or keyboard == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd23a5b-feb2-4c75-8819-fb79c6bf5320",
   "metadata": {},
   "source": [
    "## 2. Optical Flow in OpenCV\n",
    "This tutorial was originally from [here](https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de78ea4-9d53-4648-84f1-e5c231bc745a",
   "metadata": {},
   "source": [
    "Optical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Consider the image below (Image Courtesy: [Wikipedia article on Optical Flow](https://en.wikipedia.org/wiki/Optical_flow)).\n",
    "\n",
    "<img src='https://docs.opencv.org/3.4/optical_flow_basic1.jpg'/>\n",
    "image\n",
    "It shows a ball moving in 5 consecutive frames. The arrow shows its displacement vector. \n",
    "\n",
    "\n",
    "\n",
    "Optical flow works on several assumptions:\n",
    "- The pixel intensities of an object do not change between consecutive frames.\n",
    "- Neighbouring pixels have similar motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490da4eb-d166-4caa-833a-c23a92de511d",
   "metadata": {},
   "source": [
    "### Lucas-Kanade Optical Flow \n",
    "\n",
    "We have seen an assumption before, that all the neighbouring pixels will have similar motion. Lucas-Kanade method takes a 3x3 patch around the point. So all the 9 points have the same motion.\n",
    "\n",
    "So from the user point of view, the idea is simple, we give some points to track, we receive the optical flow vectors of those points. But again there are some problems. Until now, we were dealing with small motions, so it fails when there is a large motion. To deal with this we use pyramids. When we go up in the pyramid, small motions are removed and large motions become small motions. So by applying Lucas-Kanade there, we get optical flow along with the scale.\n",
    "\n",
    "\n",
    "You can see the video below for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093e4d5-4af6-4ddf-88f0-fc7d372d2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(id=\"yFX_N5p0kO0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a48fc-3848-40f2-8fd1-450afc16002a",
   "metadata": {},
   "source": [
    "To decide the points to detect optical flow, we use [cv2.goodFeaturesToTrack()](https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541). We take the first frame, detect some Shi-Tomasi corner points in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853e96-4251-48ce-9375-a45c247662d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('assets/slow_traffic_small.mp4')\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8489ff-155a-4e17-9604-60c7400a04a2",
   "metadata": {},
   "source": [
    "Then, we iteratively track those points using Lucas-Kanade optical flow. For the function [cv2.calcOpticalFlowPyrLK()](https://docs.opencv.org/3.4/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323) we pass the previous frame, previous points and next frame. It returns next points along with some status numbers which has a value of 1 if next point is found, else zero. We iteratively pass these next points as previous points in next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2e5ec-daf2-46a2-bfb7-7291ae11e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    if p1 is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "    img = cv2.add(frame, mask)\n",
    "    cv2.imshow('frame', img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0892dac-375f-412b-808b-3814ce65849f",
   "metadata": {},
   "source": [
    "#### Dense Optical Flow\n",
    "\n",
    "Lucas-Kanade method computes optical flow for a sparse feature set (in our example, corners detected using Shi-Tomasi algorithm). OpenCV provides another algorithm to find the dense optical flow. It computes the optical flow for all the points in the frame. It is based on Gunnar Farneback's algorithm which is explained in \"Two-Frame Motion Estimation Based on Polynomial Expansion\" by Gunnar Farneback in 2003.\n",
    "\n",
    "Below sample shows how to find the dense optical flow using above algorithm. We get a 2-channel array with optical flow vectors, (u,v). We find their magnitude and direction. We color code the result for better visualization. Direction corresponds to Hue value of the image. Magnitude corresponds to Value plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bba680-809b-4e90-8a9e-2de58de26a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(\"assets/slow_traffic_small.mp4\")\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('Frame', frame2)\n",
    "    cv2.imshow('frame2', bgr)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "   \n",
    "    prvs = next\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
